<!doctype html>
<html lang="en" prefix="og: http://ogp.me/ns#">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CLEAN MARKUP = GOOD KARMA.
      Hi source code lover,

      you're a curious person and a fast learner ;)
      Let's make something beautiful together. Contribute on Github:
      https://github.com/webslides/webslides

      Thanks!
    -->

    <!-- SEO -->
    <title>SC Vision</title>
    <meta name="description" content="add bnaic details here todo">

    <!-- URL CANONICAL -->
    <!-- <link rel="canonical" href="http://your-url.com/permalink"> -->

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,700,700i%7CMaitree:200,300,400,600,700&amp;subset=latin-ext" rel="stylesheet">

    <!-- CSS Base -->
    <link rel="stylesheet" type='text/css' media='all' href="../static/css/webslides.css">

    <!-- Optional - CSS SVG Icons (Font Awesome) -->
    <link rel="stylesheet" type="text/css" media="all" href="../static/css/svg-icons.css">

    <!-- SOCIAL CARDS (ADD YOUR INFO) -->

    <!-- FACEBOOK -->
    <meta property="og:url" content="http://your-url.com/permalink"> <!-- EDIT -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Make a Keynote presentation using HTML"> <!-- EDIT -->
    <meta property="og:description" content="WebSlides is the easiest way to make HTML presentations. 120+ free slides ready to use."> <!-- EDIT -->
    <meta property="og:updated_time" content="2017-01-04T17:32:14"> <!-- EDIT -->
    <meta property="og:image" content="../static/images/share-webslides.jpg" > <!-- EDIT -->

    <!-- TWITTER -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@webslides"> <!-- EDIT -->
    <meta name="twitter:creator" content="@jlantunez"> <!-- EDIT -->
    <meta name="twitter:title" content="Make a Keynote presentation using HTML"> <!-- EDIT -->
    <meta name="twitter:description" content="WebSlides is the easiest way to make HTML presentations. 120+ free slides ready to use."> <!-- EDIT -->
    <meta name="twitter:image" content="../static/images/share-webslides.jpg"> <!-- EDIT -->

    <!-- FAVICONS -->
    <link rel="shortcut icon" sizes="16x16" href="../static/images/favicons/favicon.png">
    <link rel="shortcut icon" sizes="32x32" href="../static/images/favicons/favicon-32.png">
    <link rel="apple-touch-icon icon" sizes="76x76" href="../static/images/favicons/favicon-76.png">
    <link rel="apple-touch-icon icon" sizes="120x120" href="../static/images/favicons/favicon-120.png">
    <link rel="apple-touch-icon icon" sizes="152x152" href="../static/images/favicons/favicon-152.png">
    <link rel="apple-touch-icon icon" sizes="180x180" href="../static/images/favicons/favicon-180.png">
    <link rel="apple-touch-icon icon" sizes="192x192" href="../static/images/favicons/favicon-192.png">

    <!-- Android -->
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="theme-color" content="#333333">
  </head>
  <body>
    <header role="banner">
    <p> hello </p>
    </header>

    <main role="main">
      <article id="webslides">

        <!-- Quick Guide
          - Each parent <section> in the <article id="webslides"> element is an individual slide.
          - Vertical sliding = <article id="webslides" class="vertical">
          - <div class="wrap"> = container 90% / <div class="wrap size-50"> = 45%;
        -->

        <!-- <section class="bg-apple aligncenter"> -->
          <!--.wrap = container (width: 90%) -->
          <!-- <div class="wrap"> -->
            <!-- <h1><img class="whitelogo" src="../static/images/logos/apple.svg" alt="Apple Logo"></h1> -->
          <!-- </div> -->
          <!-- .end .wrap -->
        <!-- </section> -->



        <section class="bg-white">
          <header>
            <!--.wrap o <nav> = container 1200px -->
            <div class="wrap">
              <p>20 Nov 2025 <span class="alignright"> Maastricht University </span></p>
            </div>
          </header>
          <div class="aligncenter fadeInUp">
            <h2>Self-Compressing Vision Tower for Efficient Dense Prediction Tasks</h2>
            <p>BNAIC 2025</p>
          </div>
          <footer class="bg-trans-black">
            <div class="wrap">
              <p>
                <span class="alignleft">
                  <a href="https://www.linkedin.com/in/adishourya/" target="_blank">Aditya Shourya</a>
                  <a href="https://www.linkedin.com/in/guangzhi-tang-389459a0/" target="_blank">Guangzhi Tang</a>
                  <a href="https://www.linkedin.com/in/chang-sun-maastricht/" target="_blank">Chang Sun</a>
                </span>
              </p>
            </div>
          </footer>
        </section>


        <section class="bg-white">
          <div class="wrap">
            <div class="card-50">
             <video width="800" autoplay loop muted >
              <source src="../../animations/media/videos/not_fit/1080p60/NOT_FIT.mp4" type="video/mp4">
             </video>
              <!-- <figure><img class="aligncenter" src="../static/images/iphone.png" alt="iPhone"></figure> -->
              <div class="flex-content">
              <h2>Problem</h2>
                <p class="text-intro">Big AI Models dont fit in consumer devices.</p>
              </div>
              <!-- end .flex-content-->
            </div>
            <!-- end .card-50-->
          </div>
          <!-- .end .wrap -->
        </section>

        <!-- <section class="bg-white"> -->
          <!-- <div class="wrap"> -->
            <!-- <div class="content-left"> -->
             <!-- <video width="500" autoplay loop muted > -->
              <!-- <source src="../../animations/media/videos/sollution_type/600p60/TYPE_SOLLUTION.mp4" type="video/mp4"> -->
             <!-- </video> -->
            <!-- </div> -->
            <!-- <div class="content-right"> -->
            <!-- <p> what do i write here </p> -->
            <!-- </div> -->
          <!-- </div> -->
        <!-- </section> -->

        <section class="bg-white slide-top">
          <div class="wrap">
            <div class="content-left">
             <video width="500" autoplay loop muted >
              <source src="../../animations/media/videos/trade_accuracy/1080p60/LowBitClipping.mp4" type="video/mp4">
             </video>
            </div>
            <div class="content-right">
              <h3>Trade Accuracy</h3>
              <p>Reducing accelerator memory load by clipping full-precision floats to lower-precision dtypes to improve throughput.</p>
            </div>
          </div>

        </section>

 
       
        <section class="bg-white vertical slide-bottom">
          <div class="wrap">
            <div class="content-left">
             <video width="500" autoplay loop muted >
              <source src="../../animations/media/videos/pi_buffering/1080p60/PI_BUFFERING.mp4" type="video/mp4">
             </video>
            </div>
            <div class="content-right">
              <h3>Trade Throughput</h3>
              <p>Load weights <a href="https://huggingface.co/docs/accelerate/en/index"> Just In Time </a> to perform accurate inference.</p>
            </div>
          </div>
        </section>



      <section class="bg-white">
      <div class="wrap">

            <div class="content-left">
             <video width="500" autoplay loop muted >
              <source src="../../animations/media/videos/wildlife_photography/1000p60/PhotoCaptureScene.mp4" type="video/mp4">
             </video>
            </div>

            <div class="content-right flex-content">
              <h1> Cannot Trade Either? </h1>
              <p>There are cases where throughput is as important as accuracy of inference.</p>
            </div>

          </div>
      </section>

      
      <!-- https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/ -->


      <section class="bg-white">
        <div class="wrap">
          <div class="content-left">
           <video width="600" autoplay loop muted >
            <source src="../../animations/media/videos/nvfp4_diagram/1080p60/NVFP4Diagram.mp4" type="video/mp4">
           </video>
          </div>
          <div class="content-right flex-content">
            <h2>Modern Consumer Devices</h2>
            <p>Supports <a href="https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/ " >Low Bit Precision </a> which has lower risk of accuracy drop and still offers wide dynamic tensors.</p>
          </div>
        </div>
      </section>

      
      <section class="bg-white">
        <div class="wrap">
          <div class="content-left">
           <video width="600" autoplay loop muted >
            <source src="../../animations/media/videos/quant_animation_white/1000p60/QuantizationAnimation.mp4" type="video/mp4">
           </video>
          </div>
          <div class="content-right flex-content">
            <h2>Penalize Bit Depth</h2>
            <p><a href="https://arxiv.org/abs/2301.13142" >Work </a> have been done to penalize bit depth of Weights during Quantized Aware Training.</p>

            <!-- equation -->
          </div>
              <h5>
                $$ q(W) = 2^{e} \, \text{round}\!\left(
                  \text{clip}\!\left(\frac{W}{2^{e}}, -2^{b-1}, 2^{b-1} - 1 \right)
                \right) $$
              </h5>
        </div>
      </section>

      
      
        <section class="bg-white">
          <div class="wrap">
            <div class="content-left">
             <video width="600" autoplay loop muted >
              <!-- <source src="../../animations/media/videos/hypothesis_type/600p60/TYPE_HYPOTHESIS.mp4" type="video/mp4"> -->
              <source src="../../animations/media/videos/eff_vit_diag/1080p60/TransformerBlockDiagram.mp4" type="video/mp4">
             </video>
            </div>
            <div class="content-right">
            <!-- is there a need for uniform allocation of weight bit depth and compute latency across the depth of a current state of the art--> 
            <h2>Is there a Need</h2>
            <p>
              for uniform allocation of weight bit-depth and compute latency across the depth of 
              <a href="https://github.com/mit-han-lab/efficientvit" target="_blank">
                current state-of-the-art models
              </a>
              that are often just composition of convolution and linear-attention layers?
            </p>
            </div>
          </div>
        </section>

      

        
      <section class="bg-white">
        <div class="wrap">
          <div class="content-left">
           <video width="600" autoplay loop muted >
            <source src="../../animations/media/videos/parallel_cost_white/1080p60/ParallelReduction.mp4" type="video/mp4">
           </video>
          </div>
          <div class="content-right flex-content">
            <h3>Penalize bit depth on Latency</h3>
            <p>Two Operations with same number of FLOPS might still have varying latency.</p>
          </div>
        </div>
      </section>

      
      <section class="bg-white">
        <div class="wrap">
          <div class="content-left">
           <video width="600" autoplay muted >
            <source src="../../animations/media/videos/throughput_conv/1500p60/RealLinePlot.mp4" type="video/mp4">
           </video>
          </div>
          <div class="content-right flex-content">
            <h3>Convolutionâœ¨</h3>
            <p>
              Highly parallelizable; can be performed within 
              <a href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/" target="_blank">
                a single or a few shared memory blocks
              </a>, and supports linear scheduling on most parallel accelerators.
            </p>
          </div>
        </div>
      </section>


      <section class="bg-white">
        <div class="wrap">
          <div class="content-left">
           <video width="600" autoplay muted >
            <source src="../../animations/media/videos/throughput_attn/1500p60/RealLinePlot.mp4" type="video/mp4">
           </video>
          </div>
          <div class="content-right flex-content">
            <h3>Linear Attention</h3>
            <p>
            Not as fast as convolution but Attention is <a href="https://supaiku.com/attention-is-logarithmic"> actually logarithmic </a> on resolution. Tiled MMAs are scheduled as a binary tree on a parallel accelerator. 
            </p>
          </div>
        </div>
      </section>


      
        <section class="bg-white slide-top">
          <div class="wrap">
            <div class="content-left">
             <video width="500" autoplay muted >
              <source src="../../animations/media/videos/experiment_type/600p60/TYPE_EXPERIMENT.mp4" type="video/mp4">
             </video>
            </div>
            <div class="content-right">

              <h4> Model </h4>
              <p> Quantize efficient vit </p>
              <br> <br>

              <h4> Setup </h4>
              <p> Initialize the weights to have 4 bit. </p>
              <br> <br>

              <h4> Datasets </h4>
              <p> Across Classification, segmentation and optical flow. </p>
              <br> <br>

              <h4> Eval Metrics </h4>
              <p> Across Classification, segmentation and optical flow. </p>
              <br> <br>

            </div>
          </div>
        </section>

        
      <section class="bg-white">
        <div class="wrap">
          <div class="content-left">
           <video width="700" autoplay muted >
            <source src="../../animations/media/videos/cub_dataset/1500p60/BirdGrid.mp4" type="video/mp4">
           </video>
          </div>
          <div class="content-right flex-content">
            <h3>CUB211</h3>
            <p>
            Not as fast as convolution but Attention is <a href="https://supaiku.com/attention-is-logarithmic"> actually logarithmic </a> on resolution. Tiled MMAs are scheduled as a binary tree on a parallel accelerator. 
            </p>
          </div>
        </div>
      </section>

        <section class="fullscreen bg-apple aligncenter">
         <div class="embed">
           <video autoplay muted ">
            <source src="../../animations/media/videos/table_cub/1080p60/GenerateTable.mp4" type="video/mp4">
           </video>
          </div><!-- .embed -->
        </section>


      <section class="bg-white">
        <div class="wrap">
          <div class="content-left">
           <video width="700" autoplay muted >
            <source src="../../animations/media/videos/country/1000p60/BirdGrid.mp4" type="video/mp4">
           </video>
          </div>
          <div class="content-right flex-content">
            <h3>Country211</h3>
            <p>
            Not as fast as convolution but Attention is <a href="https://supaiku.com/attention-is-logarithmic"> actually logarithmic </a> on resolution. Tiled MMAs are scheduled as a binary tree on a parallel accelerator. 
            </p>
          </div>
        </div>
      </section>

        <section class="fullscreen bg-apple aligncenter">
         <div class="embed">
           <video autoplay muted ">
            <source src="../../animations/media/videos/country_table/1080p60/GenerateTable.mp4" type="video/mp4">
           </video>
          </div><!-- .embed -->
        </section>

        
      <section class="bg-white">
        <div class="wrap">
          <div class="content-left">
           <video width="700" autoplay muted >
            <source src="../../animations/media/videos/ade20k/1000p60/BirdGrid.mp4" type="video/mp4">
           </video>
          </div>
          <div class="content-right flex-content">
            <h3>ADE20k</h3>
            <p>
            Not as fast as convolution but Attention is <a href="https://supaiku.com/attention-is-logarithmic"> actually logarithmic </a> on resolution. Tiled MMAs are scheduled as a binary tree on a parallel accelerator. 
            </p>
          </div>
        </div>
      </section>

        <section class="fullscreen bg-apple aligncenter">
         <div class="embed">
           <video autoplay muted ">
            <source src="../../animations/media/videos/ade_table/1080p60/GenerateTable.mp4" type="video/mp4">
           </video>
          </div><!-- .embed -->
        </section>

      
      <section class="bg-white">
        <div class="wrap">
          <div class="content-left">
           <video width="700" autoplay muted >
            <source src="../../animations/media/videos/hd1k/1000p60/BirdGrid.mp4" type="video/mp4">
           </video>
          </div>
          <div class="content-right flex-content">
            <h3>hd1k</h3>
            <p>
            Not as fast as convolution but Attention is <a href="https://supaiku.com/attention-is-logarithmic"> actually logarithmic </a> on resolution. Tiled MMAs are scheduled as a binary tree on a parallel accelerator. 
            </p>
          </div>
        </div>
      </section>

        <section class="fullscreen bg-apple aligncenter">
         <div class="embed">
           <video autoplay muted ">
            <source src="../../animations/media/videos/hd1k_table/1080p60/GenerateTable.mp4" type="video/mp4">
           </video>
          </div><!-- .embed -->
        </section>


          <!-- !!!!!!!!!!!!!!!!!!!!!! -->
        <!-- positioning by put slide-top / slide-bottom at the top -->
        <!-- and then div class to have content-left or content right -->
              <!-- <p><code>.slide-top/bottom and .content-left/center/right </code></p> -->
          <!-- !!!!!!!!!!!!!!!!!!!!!! -->
        


        
      <section class="bg-white">
        <div class="wrap">
          <div class="content-left">
           <video width="700" autoplay muted >
            <source src="../../animations/media/videos/bar_chart/1080p60/BitBarChart.mp4" type="video/mp4">
           </video>
          </div>
          <div class="content-right flex-content">
            <h3>Bit Depth Analysis</h3>
            <p>
            Not as fast as convolution but Attention is <a href="https://supaiku.com/attention-is-logarithmic"> actually logarithmic </a> on resolution. Tiled MMAs are scheduled as a binary tree on a parallel accelerator. 
            </p>
          </div>
        </div>
      </section>

          <section>
          <h2 class="intro">End of Presentation</h2>
          </section>

          
        <section class="fullscreen bg-apple aligncenter">
         <div class="embed">
           <video autoplay loop muted ">
            <source src="../../animations/media/videos/tying_conv1/1080p60/compressKernels.mp4" type="video/mp4">
           </video>
          </div><!-- .embed -->
        </section>

          

      </article>
    </main>
    <!--main-->

    <!-- Required -->
    <script src="../static/js/webslides.js"></script>

    
    <!-- hope this isnt heavy -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <script>
      window.ws = new WebSlides();
    </script>

    <!-- OPTIONAL - svg-icons.js (fontastic.me - Font Awesome as svg icons) -->
    <script defer src="../static/js/svg-icons.js"></script>

  </body>
</html>
